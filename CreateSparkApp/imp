// https://spark.apache.org/docs/1.1.0/mllib-clustering.html
// spark-shell SPARK_CLASSPATH=/Volumes/DataDrive/dev/aas/kmeans/target/kmeans-1.0.0-jar-with-dependencies.jar

val rawData = sc.textFile("/home/ubuntu/kddcup.data", 120)
val rawData = sc.textFile("/Users/antigen/dev/kddcup.sample", 120)

rawData.count
rawData.take(1)


val dataAndLabel = rawData.map { line =>
  val buffer = line.split(",").toBuffer
  buffer.remove(1,3)
  val label = buffer.remove(buffer.length-1)
  val vector = buffer.map(_.toDouble).toArray
  (vector, label)
}

val data = dataAndLabel.map(_._1).cache()

import org.apache.spark.mllib.clustering._
val kmeans = new KMeans()
val model = kmeans.run(data)

val clusterLabelCount = dataAndLabel.map {
	case (data,label) => (model.predict(data), label)
  }.countByValue.toList.sorted.foreach {
    case ((cluster,label),count) =>
      println(f"$cluster5ls$label%18s$count%8s")
  }


// take 2
import scala.math
import org.apace.spark.rdd._


def distance(a: Array[Double], b:Array[Double]) = 
  sqrt(a.zip(b).map(p> p._1 - p._2).map( d=> d*d).sum)

def distToCentroid(datum: Array[Double], model: KMeansModel)  =
  distance(model.clusterCenters(model.predict(dataum)),datum)

def clusteringScore(data: RDD[Array[Double]], k: Int) = {
	val kmeans = new KMeans()
	kmeans.setK(k)
	val model = kmeans.run(data)
	data.map(datum => distToCentroid(datum, model)).mean
}

val kScores = (5 to 40 by 5).par.map(k = >
	(k, clusteringScore(data, k)))


// take 3 normalizing noisy data with big numbers that destroy clusters
val numCols = data.take(1)(0).length

val n = data.count

val sums = data.reduce((a,b) =>
	a.zip(b).map(t=> t._1 + t._2))

val sumSquares =  data.fold(new Array[Double](numCols))
    ((a,b) => a.zip(b).map( t => t._1 + t._2 * t._2))


val stddevs = sumSquares.zip(sums).map {
	case(sumSq,sum) => sqrt(n*sumSq - sum*sum)/n
}

val means = sums.map(_ /n)

def normalize(f:Array[Double]) =
  (f,means,stdevs).zipped.map((value,mean,stdev) =>
  	if (stdev <= 0) (value-mean) else (value-mean)/stdev)

val normalizedData = data.map(normalize(_)).cache()

val kScores = (50 to 120 by 10).par.map( k=>
    (k, clusteringScore(normalizedData, k)))


//take 4 takin care of categorical imofrmation to help classificaiton

val protocols = rawData.map (
_.split(",")(1).distinct.collect.zipWithIndex.toMap
	)

val dataAndLabel = rawData.map { line =>
   val buffer  = line.split(",").toBuffer
   val protocol = buffer.remove(1)
   val vector = buffer.map(_.toDouble)

   val newProtocolFeatures = new Array[Double](protocols.size) newProtocolFeatures(protocols(protocol) = 1.0)

   vector.insertAll(1, newProtocolFeatures)

   (vector.toArray,label) 
}

//take five  - using the labels and entropy(means length) to figure make the kmeans model more accurate
def entropy(counts: Iterable[Int]) = {
    val values = counts.filter(_ > 0)
    val sum: Double = values.sum
    values.map { v=>
      val p = v/sum - p * log(p)
    }.sum
}

def clusteringScore(data: RDD[Array[Double]], labels: RDD[String], k: Int) = {

	val labelsInCluster = data.map(model.predict(_)).zip(labels).groupByKey.values
    val labelCounts = labelsInCluster.map(_.groupBy( 1=> 1).map(t -> t._2.length))
    val n = data.count
    labelCounts.map( m= m.sum * entropy(m)),sum / n 
}



val kmeans = new KMeans()
kmeans.setK(95)
kmeans.setRuns(10)
kmeans.setEpsilo(1.0.e-6)
val model = kmeans.run(normalizedData)

val distances = normalizedData.map(datum =>
	(distToCentroid(datum, model), datum))

val outliers = distance.top(100)(Ordering.by(_._1))
val treshold = outliers.last._1

def anomaly(datum: Array[Double], model:KMeansModel) = 
   distToCentroid(normalize(datum), model) > threshold




/*
 * Copyright 2015 Sanford Ryza, Uri Laserson, Sean Owen and Joshua Wills
 *
 * See LICENSE file for further information.
 */

package com.cloudera.datascience.kmeans

import org.apache.spark.mllib.clustering._
import org.apache.spark.mllib.linalg._
import org.apache.spark.rdd._
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._

object RunKMeans {

  def main(args: Array[String]): Unit = {
    val sc = new SparkContext(new SparkConf().setAppName("K-means"))
    val rawData = sc.textFile("hdfs:///user/ds/kddcup.data")
    clusteringTake0(rawData)
    clusteringTake1(rawData)
    clusteringTake2(rawData)
    clusteringTake3(rawData)
    clusteringTake4(rawData)
    anomalies(rawData)
  }

  // Clustering, Take 0

  def clusteringTake0(rawData: RDD[String]): Unit = {

    rawData.map(_.split(',').last).countByValue().toSeq.sortBy(_._2).reverse.foreach(println)

    val labelsAndData = rawData.map { line =>
      val buffer = line.split(',').toBuffer
      buffer.remove(1, 3)
      val label = buffer.remove(buffer.length - 1)
      val vector = Vectors.dense(buffer.map(_.toDouble).toArray)
      (label, vector)
    }

    val data = labelsAndData.values.cache()

    val kmeans = new KMeans()
    val model = kmeans.run(data)

    model.clusterCenters.foreach(println)

    val clusterLabelCount = labelsAndData.map { case (label, datum) =>
      val cluster = model.predict(datum)
      (cluster, label)
    }.countByValue()

    clusterLabelCount.toSeq.sorted.foreach { case ((cluster, label), count) =>
      println(f"$cluster%1s$label%18s$count%8s")
    }

    data.unpersist()
  }

  // Clustering, Take 1

  def distance(a: Vector, b: Vector) =
    math.sqrt(a.toArray.zip(b.toArray).map(p => p._1 - p._2).map(d => d * d).sum)

  def distToCentroid(datum: Vector, model: KMeansModel) = {
    val cluster = model.predict(datum)
    val centroid = model.clusterCenters(cluster)
    distance(centroid, datum)
  }

  def clusteringScore(data: RDD[Vector], k: Int): Double = {
    val kmeans = new KMeans()
    kmeans.setK(k)
    val model = kmeans.run(data)
    data.map(datum => distToCentroid(datum, model)).mean()
  }

  def clusteringScore2(data: RDD[Vector], k: Int): Double = {
    val kmeans = new KMeans()
    kmeans.setK(k)
    kmeans.setRuns(10)
    kmeans.setEpsilon(1.0e-6)
    val model = kmeans.run(data)
    data.map(datum => distToCentroid(datum, model)).mean()
  }

  def clusteringTake1(rawData: RDD[String]): Unit = {

    val data = rawData.map { line =>
      val buffer = line.split(',').toBuffer
      buffer.remove(1, 3)
      buffer.remove(buffer.length - 1)
      Vectors.dense(buffer.map(_.toDouble).toArray)
    }.cache()

    // blows up at the 25 and 30 bins
    (5 to 30 by 5).map(k => (k, clusteringScore(data, k))).
      foreach(println)

    // blows up too
    (30 to 100 by 10).par.map(k => (k, clusteringScore2(data, k))).
      toList.foreach(println)

    data.unpersist()

  }

  def visualizationInR(rawData: RDD[String]): Unit = {

    val data = rawData.map { line =>
      val buffer = line.split(',').toBuffer
      buffer.remove(1, 3)
      buffer.remove(buffer.length - 1)
      Vectors.dense(buffer.map(_.toDouble).toArray)
    }.cache()

    val kmeans = new KMeans()
    kmeans.setK(100)
    kmeans.setRuns(10)
    kmeans.setEpsilon(1.0e-6)
    val model = kmeans.run(data)

    val sample = data.map(datum =>
      model.predict(datum) + "," + datum.toArray.mkString(",")
    ).filter(_.hashCode % 20 == 0)

    sample.saveAsTextFile("hdfs:///user/ds/sample")

    data.unpersist()

  }

  // Clustering, Take 2

  def buildNormalizationFunction(data: RDD[Vector]): (Vector => Vector) = {
    val dataAsArray = data.map(_.toArray)
    val numCols = dataAsArray.first().length
    val n = dataAsArray.count()
    val sums = dataAsArray.reduce(
      (a, b) => a.zip(b).map(t => t._1 + t._2))
    val sumSquares = dataAsArray.fold(
        new Array[Double](numCols)
      )(
        (a, b) => a.zip(b).map(t => t._1 + t._2 * t._2)
      )
    val stdevs = sumSquares.zip(sums).map {
      case (sumSq, sum) => math.sqrt(n * sumSq - sum * sum) / n
    }
    val means = sums.map(_ / n)

    (datum: Vector) => {
      val normalizedArray = (datum.toArray, means, stdevs).zipped.map(
        (value, mean, stdev) =>
          if (stdev <= 0)  (value - mean) else  (value - mean) / stdev
      )
      Vectors.dense(normalizedArray)
    }
  }

  def clusteringTake2(rawData: RDD[String]): Unit = {
    val data = rawData.map { line =>
      val buffer = line.split(',').toBuffer
      buffer.remove(1, 3)
      buffer.remove(buffer.length - 1)
      Vectors.dense(buffer.map(_.toDouble).toArray)
    }

    val normalizedData = data.map(buildNormalizationFunction(data)).cache()

    (60 to 120 by 10).par.map(k =>
      (k, clusteringScore2(normalizedData, k))).toList.foreach(println)

    normalizedData.unpersist()
  }

  // Clustering, Take 3

  def buildCategoricalAndLabelFunction(rawData: RDD[String]): (String => (String,Vector)) = {
    val protocols = rawData.map(_.split(',')(1)).distinct().collect().zipWithIndex.toMap
    val services = rawData.map(_.split(',')(2)).distinct().collect().zipWithIndex.toMap
    val tcpStates = rawData.map(_.split(',')(3)).distinct().collect().zipWithIndex.toMap
    (line: String) => {
      val buffer = line.split(",").toBuffer
      val protocol = buffer.remove(1)
      val service = buffer.remove(1)
      val tcpState = buffer.remove(1)
      val label = buffer.remove(buffer.length - 1)
      val vector = buffer.map(_.toDouble)

      val newProtocolFeatures = new Array[Double](protocols.size)
      newProtocolFeatures(protocols(protocol)) = 1.0
      val newServiceFeatures = new Array[Double](services.size)
      newServiceFeatures(services(service)) = 1.0
      val newTcpStateFeatures = new Array[Double](tcpStates.size)
      newTcpStateFeatures(tcpStates(tcpState)) = 1.0

      vector.insertAll(1, newTcpStateFeatures)
      vector.insertAll(1, newServiceFeatures)
      vector.insertAll(1, newProtocolFeatures)

      (label, Vectors.dense(vector.toArray))
    }
  }

  def clusteringTake3(rawData: RDD[String]): Unit = {
    val parseFunction = buildCategoricalAndLabelFunction(rawData)
    val data = rawData.map(parseFunction).values
    val normalizedData = data.map(buildNormalizationFunction(data)).cache()

    (80 to 160 by 10).map(k =>
      (k, clusteringScore2(normalizedData, k))).toList.foreach(println)

    normalizedData.unpersist()
  }

  // Clustering, Take 4

  def entropy(counts: Iterable[Int]) = {
    val values = counts.filter(_ > 0)
    val n: Double = values.sum
    values.map { v =>
      val p = v / n
      -p * math.log(p)
    }.sum
  }

  def clusteringScore3(normalizedLabelsAndData: RDD[(String,Vector)], k: Int) = {
    val kmeans = new KMeans()
    kmeans.setK(k)
    kmeans.setRuns(10)
    kmeans.setEpsilon(1.0e-6)
    val model = kmeans.run(normalizedLabelsAndData.values)
    // Predict cluster for each datum
    val labelsAndClusters = normalizedLabelsAndData.mapValues(model.predict)
    // Swap keys / values
    val clustersAndLabels = labelsAndClusters.map(_.swap)
    // Extract collections of labels, per cluster
    val labelsInCluster = clustersAndLabels.groupByKey().values
    // Count labels in collections
    val labelCounts = labelsInCluster.map(_.groupBy(l => l).map(_._2.size))
    // Average entropy weighted by cluster size
    val n = normalizedLabelsAndData.count()
    labelCounts.map(m => m.sum * entropy(m)).sum / n
  }

  def clusteringTake4(rawData: RDD[String]): Unit = {
    val parseFunction = buildCategoricalAndLabelFunction(rawData)
    val labelsAndData = rawData.map(parseFunction)
    val normalizedLabelsAndData =
      labelsAndData.mapValues(buildNormalizationFunction(labelsAndData.values)).cache()

    (80 to 160 by 10).map(k =>
      (k, clusteringScore3(normalizedLabelsAndData, k))).toList.foreach(println)

    normalizedLabelsAndData.unpersist()
  }

  // Detect anomalies

  def buildAnomalyDetector(
      data: RDD[Vector],
      normalizeFunction: (Vector => Vector)): (Vector => Boolean) = {
    val normalizedData = data.map(normalizeFunction)
    normalizedData.cache()

    val kmeans = new KMeans()
    kmeans.setK(150)
    kmeans.setRuns(10)
    kmeans.setEpsilon(1.0e-6)
    val model = kmeans.run(normalizedData)

    normalizedData.unpersist()

    val distances = normalizedData.map(datum => distToCentroid(datum, model))
    val threshold = distances.top(100).last

    (datum: Vector) => distToCentroid(normalizeFunction(datum), model) > threshold
  }

  def anomalies(rawData: RDD[String]) = {
    val parseFunction = buildCategoricalAndLabelFunction(rawData)
    val originalAndData = rawData.map(line => (line, parseFunction(line)._2))
    val data = originalAndData.values
    val normalizeFunction = buildNormalizationFunction(data)
    val anomalyDetector = buildAnomalyDetector(data, normalizeFunction)
    val anomalies = originalAndData.filter(
      originalAndDatum => anomalyDetector(originalAndDatum._2)
    ).keys
    anomalies.take(10).foreach(println)
  }

}


// https://gist.github.com/bigsnarfdude/baf15b8c7eb985f8241b 


