import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.rdd._
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.linalg.distributed.RowMatrix
import org.apache.spark.mllib.linalg.Matrices
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.feature.PCA
import scala.collection.immutable.ListMap


object LOF {
  def main(args: Array[String]) {
    val conf = new SparkConf()
        conf.setAppName("LOF")
        conf.set("spark.storage.memoryFraction", "1");
    val sc = new SparkContext(conf)
    
     val metadata = sc.textFile("/data/kddcupdata/kddcup.trasfrom")
        
    // train data only normal instances
    val data = sc.textFile("/data/kddcupdata/kddcup.trasfrom.normal.1")

    // test data only normal instances to run on LOF generated by building model 
    val datatwo = sc.textFile("/data/kddcupdata/kddcup.trasfrom.normal.2")

    // test data everything 
    val testData =  sc.textFile("/data/kddcupdata/correctednoicmp")


    // for feature expansion 
    val protocols = metadata.map(_.split(',')(1)).distinct().collect().zipWithIndex.toMap
    val services = metadata.map(_.split(',')(2)).distinct().collect().zipWithIndex.toMap
    val tcpStates = metadata.map(_.split(',')(3)).distinct().collect().zipWithIndex.toMap

    val trainData = data.map{line =>
            val buffer = line.split(",").toBuffer
            val protocol = buffer.remove(1)
            val service = buffer.remove(1)
            val tcpState = buffer.remove(1)
            val label = buffer.remove(buffer.length - 1)
            val vector = buffer.map(_.toDouble)

            val newProtocolFeatures = new Array[Double](protocols.size)
            newProtocolFeatures(protocols(protocol)) = 1.0
            val newServiceFeatures = new Array[Double](services.size)
            newServiceFeatures(services(service)) = 1.0
            val newTcpStateFeatures = new Array[Double](tcpStates.size)
            newTcpStateFeatures(tcpStates(tcpState)) = 1.0

            vector.insertAll(1, newTcpStateFeatures)
            vector.insertAll(1, newServiceFeatures)
            vector.insertAll(1, newProtocolFeatures)
            
            var classlabel = 0.0 
            if(label != "normal.") {
                classlabel = 1.0
            }
             //(label,Vectors.dense(vector.t
            new LabeledPoint(classlabel, Vectors.dense(vector.toArray))
          
            
    }


    // do pca 
    val pcaK = 20
    val pca = new PCA(pcaK).fit(trainData.map(_.features))


    // project train data to pca
    val projectedTrainData = trainData.map(p => p.copy(features = pca.transform(p.features)))
    val kmeanTraindata = projectedTrainData.map(line  => (line.features)) 

    // do k-means clustering 
    // val numClusters = 100
    val numClusters = args(0).toInt
    val numIterations = 10     
    val kmeans = new KMeans()
    kmeans.setK(numClusters)
    kmeans.setRuns(numIterations)
    val model = kmeans.run(kmeanTraindata)


    // save the cluster centers in a map to use it later
    var clusterID =0
    var centroidMap:Map[Int,Vector] = Map() 
    // build a map to store the count of points in all the cluster
    var pointsCountMap:Map[Int, Long] = Map()
    var pointsCountMapTwo:Map[Int, Long] = Map()
    model.clusterCenters.map{ line =>
        // print("cluster" + line)
        centroidMap += ( clusterID -> line )
       //  pointsCountMap += ( clusterID -> 0 )
        clusterID += 1
    }

    

    // build a map to store the count of points in all the cluster 
    // var pointsCountMap:Map[Int, Int] = Map()
    val pointsPerCluster = projectedTrainData.map{line =>
           
           var clusterID = model.predict(line.features)
        (clusterID)
    }.countByValue
    
    pointsPerCluster.toSeq.sorted.foreach {
            case((clusterID),count) =>
            pointsCountMap += (clusterID -> count)
        }
 
  
  //  centroidMap.foreach(println)
    println("centroid map is done")
    val min_pts = args(1).toInt
    // build the outlier factor for every training data point in parallel by calling function 
    // local_outlier_factor
    // get local outlier degree of all train data 
    val trainOutlierDegree = projectedTrainData.map{line =>
        var outlierFactor = local_outlier_factor(min_pts,line.features,centroidMap,pointsCountMap)
        (outlierFactor,1)
     }

     // trainOutlierDegree.foreach(println)


    // take first n% to decide threshold
     var thresholdPercent = args(2).toInt 
     var firstN =  ((data.count() * thresholdPercent)/100).toInt 
     val sortedData = trainOutlierDegree.sortByKey(false,numPartitions = 1).collect()
     sortedData.foreach(println)
     var threshold1 = sortedData.take(firstN).last.toString.split(',')(0).substring(1)
     println(threshold1)
     var threshold = threshold1.toDouble
    

    // read 2nd half of data 
     val secondData = datatwo.map{line =>
            val buffer = line.split(",").toBuffer
            val protocol = buffer.remove(1)
            val service = buffer.remove(1)
            val tcpState = buffer.remove(1)
            val label = buffer.remove(buffer.length - 1)
            val vector = buffer.map(_.toDouble)

            val newProtocolFeatures = new Array[Double](protocols.size)
            newProtocolFeatures(protocols(protocol)) = 1.0
            val newServiceFeatures = new Array[Double](services.size)
            newServiceFeatures(services(service)) = 1.0
            val newTcpStateFeatures = new Array[Double](tcpStates.size)
            newTcpStateFeatures(tcpStates(tcpState)) = 1.0

            vector.insertAll(1, newTcpStateFeatures)
            vector.insertAll(1, newServiceFeatures)
            vector.insertAll(1, newProtocolFeatures)

            var classlabel = 0.0
            if(label != "normal.") {
                classlabel = 1.0
            }
             //(label,Vectors.dense(vector.t
            new LabeledPoint(classlabel, Vectors.dense(vector.toArray))


    }
    

    // project 2nd part of train  data to pca
    val projectedSecondData = secondData.map(p => p.copy(features = pca.transform(p.features)))
    
     val traindatawithoutlier = projectedSecondData.map{line =>
        var outlierDegree = local_outlier_factor(min_pts,line.features,centroidMap,pointsCountMap)

        (line.features,outlierDegree)

        // org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] 
     } 

      val traindatawithoutoutlier = traindatawithoutlier.filter{ case(line,value) => value > threshold }
    
     val traindatamodel2 = traindatawithoutoutlier.keys 
    

     // do k-means clustering
    // val numClusters = 100
    val numClusterstwo = args(0).toInt
    val numIterationstwo = 10
    val kmeanstwo = new KMeans()
    kmeanstwo.setK(numClusterstwo)
    kmeanstwo.setRuns(numIterationstwo)
    val modeltwo = kmeanstwo.run(traindatamodel2)


    // save the cluster centers in a map to use it later
    var clusterIDtwo =0
    var centroidMaptwo:Map[Int,Vector] = Map()
    // build a map to store the count of points in all the cluster
    var pointsCountMaptwo:Map[Int, Long] = Map()
    modeltwo.clusterCenters.map{ line =>
        // print("cluster" + line)
        centroidMaptwo += ( clusterID -> line )
       //  pointsCountMap += ( clusterID -> 0 )
        clusterIDtwo += 1
    }



    // build a map to store the count of points in all the cluster
    // var pointsCountMap:Map[Int, Int] = Map()
    val pointsPerClustertwo = traindatamodel2.map{line =>

           var clusterID = modeltwo.predict(line)
        (clusterID)
    }.countByValue

    pointsPerClustertwo.toSeq.sorted.foreach {
            case((clusterID),count) =>
            pointsCountMapTwo += (clusterID -> count)
        }


    println("centroid two map is done")
    val min_ptstwo = args(1).toInt 
    // build the outlier factor for every training data point in parallel by calling function
    // local_outlier_factor
    // get local outlier degree of all train data 
    val trainOutlierDegreetwo = traindatamodel2.map{line =>
        var outlierFactor = local_outlier_factor(min_pts,line,centroidMaptwo,pointsCountMaptwo)
        (outlierFactor,1)
     }
     
     // trainOutlierDegree.foreach(println)
    
    
    // take first n% to decide threshold
     var thresholdPercenttwo = args(2).toInt 
     var firstNtwo =  ((data.count() * thresholdPercenttwo)/100).toInt 
     val sortedDatatwo = trainOutlierDegreetwo.sortByKey(false,numPartitions = 1).collect()
     sortedData.foreach(println)
     var threshold2 = sortedDatatwo.take(firstNtwo).last.toString.split(',')(0).substring(1)
     println(threshold2)
     var threshold21 = threshold2.toDouble
    

    // read test data and build features 
    
    val labelTestData = testData.map{line => 
            val buffer = line.split(",").toBuffer
            val protocol = buffer.remove(1)
            val service = buffer.remove(1)
            val tcpState = buffer.remove(1)
            val label = buffer.remove(buffer.length - 1)
            val vector = buffer.map(_.toDouble)

            val newProtocolFeatures = new Array[Double](protocols.size)
            newProtocolFeatures(protocols(protocol.trim)) = 1.0
            val newServiceFeatures = new Array[Double](services.size)
            newServiceFeatures(services(service.trim)) = 1.0
            val newTcpStateFeatures = new Array[Double](tcpStates.size)
            newTcpStateFeatures(tcpStates(tcpState.trim)) = 1.0
            vector.insertAll(1, newTcpStateFeatures)
            vector.insertAll(1, newServiceFeatures)
            vector.insertAll(1, newProtocolFeatures)
            // (label,Vectors.dense(vector.toArray))

            var classlabel = 0.0 
            if(label != "normal.") {
                classlabel = 1.0
            }
       

        new LabeledPoint(classlabel, Vectors.dense(vector.toArray))
            
   }

    // project test data to pca
    val projectedTestData = labelTestData.map(p => p.copy(features = pca.transform(p.features)))

    // check for test data 

    val testOutlierDegree = projectedTestData.map{line =>
        var outlierDegree = local_outlier_factor(min_pts,line.features,centroidMap,pointsCountMap)
       // println(line.label+","+outlierDegree)
        var classLabel = "noclass"
        if(line.label == 0.0 && outlierDegree < threshold21) {
            // println("TP")
            classLabel = "TP"
        } else if(line.label != 0.0 && outlierDegree < threshold21) {
            classLabel = "FP"
        } else if(line.label == 0.0 && outlierDegree > threshold21) {
            classLabel = "FN"
        } else if(line.label != 0.0 && outlierDegree > threshold21) {
            classLabel = "TN"
        }
        (classLabel)
     }.countByValue
    
    // testOutlierDegree.foreach(println)   

     testOutlierDegree.toSeq.sorted.foreach {
            case((classLabel),count) =>
             // println(f"$cluster%1s$label%18s$count%8s")
             println(classLabel +" : Count :" + count)
        } 
   
  
    }


    /*
        function to calculate k-distance of a point 
    */

    def k_distance(k: Int , instance : Vector, centoridMap : Map[Int,Vector])  = {
        // println("instance in k-distance is " + instance )
        var distances:Map[Double,Vector] = Map() 
        var distanceIDMap : Map[Double,Int] = Map()
        for ((k,v) <- centoridMap) {
            var distance_value = Math.sqrt(Vectors.sqdist(instance, v))
            distances += ( distance_value -> v )
            distanceIDMap += (distance_value -> k)
        }
        var distanceList = distances.toList.sortBy (_._1) 
        var neighbours = new Array[Vector](k)
        var neighboursClusterID = new Array[Int](k)
        var k_distance = 0.0
        var index = 0
        for((k1,v) <- distances.take(k) ) {
            neighbours(index) = v
            neighboursClusterID(index) = distanceIDMap(k1) // store the cluster id of k nearest neighbors
            if(index == k-1) {
                k_distance = k1
            }
            index += 1
        }
        // print("k-distacce is : "+ k_distance)
        //[neighbours.extend(n[1]) for n in distances[:k]]
        (k_distance, neighbours,neighboursClusterID)
    }


    /*
        function to calculate reachability distance of a point , which is maximum of k-distance and 
    */

    def reachability_distance(k : Int, instance1 : Vector , instance2 : Vector, centoridmap : Map[Int,Vector])  = {
         // get the k distance of the instance with its neighbors
         val (k_distance_value, neighbours,neighboursClusterID) = k_distance(k, instance2, centoridmap)
         // reachability distance is maximum distance between the k-th distance and euclidean distance 
        (Math.max(k_distance_value,Math.sqrt(Vectors.sqdist(instance1,instance2))))

    }


    /*
        function to calculate LRD , local reachable density of a point 
    */


    def local_reachability_density(min_pts : Int , instance : Vector, centroidMap : Map[Int,Vector],pointsCountMap : Map[Int,Long]) = {
        // get the k-distacne value and all k neighbors of given instance
        val (k_distance_value, neighbours,neighboursClusterID) = k_distance(min_pts, instance, centroidMap)
        var reachability_distances_array = Array.fill(neighbours.length){0.0} 
        for (i <- 0 until neighbours.length -1) {
            reachability_distances_array(i) = reachability_distance(min_pts, instance, neighbours(i),centroidMap)
        }

         (neighbours.length/reachability_distances_array.sum)


    }

    
    /*
        function to calculate local outlier factor for a given point 
    */

    def local_outlier_factor(min_pts  : Int , instance: Vector, centoridmap : Map[Int,Vector] , pointsCountMap : Map[Int,Long]) = {
        // first calculate k-distance for the given point 
        val (k_distance_value, neighbours,neighboursClusterID) = k_distance(min_pts, instance , centoridmap)
        // weighting mechanism 
        // first calculate total number of points in min_points clusters
        var totalPoints = 0.0
        for(i<-0  until neighboursClusterID.length -1) {
            totalPoints += pointsCountMap(neighboursClusterID(i))
        }

        // calculate lrd value for the given point 
        val instance_lrd = local_reachability_density(min_pts, instance, centoridmap,pointsCountMap)

        val lrd_ratios_array = Array.fill(neighbours.length){0.0} 

        // calculate lrd for all instance point for all the cluster centers 

        for( i <- 0 until neighbours.length - 1) {
            // we have to remove the processing row from map 
           // println(" processing neighbor " + neighbours(i))
            val neighbour_lrd  = local_reachability_density(min_pts, neighbours(i), centoridmap,pointsCountMap)
            lrd_ratios_array(i) = ( (pointsCountMap(neighboursClusterID(i)) / totalPoints )* neighbour_lrd)/instance_lrd
        }

        (lrd_ratios_array.sum / neighbours.length)

    }

   
 }


